{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6858ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from typing import List\n",
    "from pandas_datareader import data as pdr\n",
    "import gudhi as gd\n",
    "\n",
    "# Variables globales\n",
    "\n",
    "TICKERS = [\"AAPL\",\"MSFT\",\"GOOGL\",\"AMZN\",\"NVDA\",\"META\",\"TSM\",\"LLY\",\"BRK-B\",\"NVO\"]\n",
    "START = \"2019-01-01\"\n",
    "END   = \"2025-11-01\"\n",
    "\n",
    "OUT_DIR = \"./data\"\n",
    "OUT_PARQUET = \"market_panel.parquet\"\n",
    "\n",
    "AUTO_ADJUST = True   # yfinance: ajusta por splits/dividendos\n",
    "BACK_ADJUST = False\n",
    "\n",
    "INCLUDE_VIX = True\n",
    "RV_WINDOWS = (5, 10, 21)   # días hábiles\n",
    "\n",
    "# Mapeo estático simple de sector (puedes refinarlo con una fuente externa)\n",
    "SECTOR_MAP = {\n",
    "    \"AAPL\": \"Tech\", \"MSFT\": \"Tech\", \"GOOGL\": \"Tech\", \"AMZN\": \"ConsDisc\",\n",
    "    \"NVDA\": \"Tech\", \"META\": \"CommServ\", \"TSM\": \"Semis\", \"LLY\": \"Health\",\n",
    "    \"BRK-B\": \"Financials\", \"NVO\": \"Health\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "619ab163",
   "metadata": {},
   "outputs": [],
   "source": [
    "def asegurar_directorio_salida(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "\n",
    "def download_adjusted_closes(\n",
    "    tickers: List[str],\n",
    "    start: str,\n",
    "    end: str,\n",
    "    auto_adjust: bool = True,\n",
    "    threads: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Descarga precios 'Close' ajustados (dividendos/splits) desde Yahoo Finance.\n",
    "    Devuelve un DataFrame con índice datetime y una columna por ticker.\n",
    "    \"\"\"\n",
    "    if yf is None:\n",
    "        raise ImportError(\"yfinance no está disponible. Instala con: pip install yfinance\")\n",
    "\n",
    "    if not tickers:\n",
    "        raise ValueError(\"La lista de tickers no puede estar vacía.\")\n",
    "\n",
    "    data = yf.download(\n",
    "        tickers,\n",
    "        start=start,\n",
    "        end=end,\n",
    "        auto_adjust=auto_adjust,\n",
    "        progress=False,\n",
    "        group_by=\"ticker\",\n",
    "        threads=threads,\n",
    "    )\n",
    "\n",
    "    if isinstance(tickers, str) or len(tickers) == 1:\n",
    "        t = tickers[0] if isinstance(tickers, list) else tickers\n",
    "        closes = data[\"Close\"].to_frame(name=t)\n",
    "    else:\n",
    "        closes = pd.DataFrame({t: data[t][\"Close\"] for t in tickers})\n",
    "    closes = closes.sort_index().dropna(how=\"all\")\n",
    "    closes.index = pd.to_datetime(closes.index)\n",
    "\n",
    "    # Opcional: asegurar zona horaria naive\n",
    "    closes.index = closes.index.tz_localize(None) if getattr(closes.index, \"tz\", None) else closes.index\n",
    "    return closes\n",
    "\n",
    "def closes_wide_to_long_panel(closes_wide):\n",
    "    \"\"\"\n",
    "    Convierte el DataFrame ancho (index: ds, cols: tickers con Close ajustado)\n",
    "    a formato largo Nixtla-style con columnas: [unique_id, ds, Close].\n",
    "    \"\"\"\n",
    "    if not isinstance(closes_wide.index, pd.DatetimeIndex):\n",
    "        closes_wide.index = pd.to_datetime(closes_wide.index)\n",
    "\n",
    "    \n",
    "    # Mantén filas aunque haya NaNs (ya filtraremos luego)\n",
    "    long = (closes_wide\n",
    "            .stack(dropna=False)                # MultiIndex (ds, unique_id)\n",
    "            .rename(\"Close\")\n",
    "            .reset_index()\n",
    "            .rename(columns={\"level_0\":\"ds\", \"level_1\":\"unique_id\"}))\n",
    "    # Tipos consistentes\n",
    "    long[\"ds\"] = pd.to_datetime(long[\"Date\"])\n",
    "    long[\"unique_id\"] = long[\"unique_id\"].astype(str)\n",
    "    long[\"Close\"] = pd.to_numeric(long[\"Close\"], errors=\"coerce\")\n",
    "\n",
    "    # Orden canónico\n",
    "    long = long.sort_values([\"unique_id\",\"ds\"]).reset_index(drop=True)\n",
    "    return long\n",
    "\n",
    "\n",
    "# 2) AJUSTE: calcular_logret (quita print de debug)\n",
    "def calcular_logret(df, price_col=\"Close\"):\n",
    "    out = df.copy()\n",
    "    out = out.loc[:, ~out.columns.duplicated()].copy()\n",
    "\n",
    "    if price_col not in out.columns:\n",
    "        raise KeyError(f\"No se encontró columna '{price_col}' en el DataFrame.\")\n",
    "\n",
    "    out = out.sort_values([\"unique_id\", \"ds\"]).copy()\n",
    "    out[price_col] = pd.to_numeric(out[price_col], errors=\"coerce\")\n",
    "\n",
    "    prev = out.groupby(\"unique_id\", sort=False)[price_col].shift(1)\n",
    "    out[\"logret_raw\"] = np.log(out[price_col] / prev)\n",
    "    out.loc[~np.isfinite(out[\"logret_raw\"]), \"logret_raw\"] = np.nan\n",
    "\n",
    "    out[\"logret_scaled\"] = np.nan\n",
    "    for uid, idx in out.groupby(\"unique_id\", sort=False).groups.items():\n",
    "        vals = out.loc[idx, \"logret_raw\"].to_numpy()\n",
    "        mask = np.isfinite(vals)\n",
    "        if mask.sum() <= 1:\n",
    "            out.loc[idx, \"logret_scaled\"] = vals\n",
    "            continue\n",
    "        x = vals[mask]\n",
    "        med = np.median(x)\n",
    "        q75 = np.percentile(x, 75)\n",
    "        q25 = np.percentile(x, 25)\n",
    "        iqr = q75 - q25\n",
    "        if not np.isfinite(iqr) or iqr == 0:\n",
    "            std = np.std(x)\n",
    "            if not np.isfinite(std) or std == 0:\n",
    "                std = 1.0\n",
    "            scaled = (vals - med) / std\n",
    "        else:\n",
    "            scaled = (vals - med) / iqr\n",
    "        out.loc[idx, \"logret_scaled\"] = scaled\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "def agregar_calendario(df):\n",
    "    \"\"\"\n",
    "    futr_exog derivadas de la fecha.\n",
    "    Arreglo:\n",
    "      - Corregido 'dt' (antes estaba 't') y 'is_quarter_start' compatible.\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "    out[\"ds\"] = pd.to_datetime(out[\"ds\"])\n",
    "\n",
    "    out[\"dow\"]   = out[\"ds\"].dt.weekday.astype(\"int16\")\n",
    "    out[\"dom\"]   = out[\"ds\"].dt.day.astype(\"int16\")\n",
    "    out[\"woy\"]   = out[\"ds\"].dt.isocalendar().week.astype(\"int16\")\n",
    "    out[\"month\"] = out[\"ds\"].dt.month.astype(\"int16\")\n",
    "    out[\"qtr\"]   = out[\"ds\"].dt.quarter.astype(\"int16\")\n",
    "    out[\"eom\"]   = out[\"ds\"].dt.is_month_end.astype(\"int8\")\n",
    "    out[\"eoq\"]   = out[\"ds\"].dt.is_quarter_end.astype(\"int8\")\n",
    "    out[\"eoy\"]   = out[\"ds\"].dt.is_year_end.astype(\"int8\")\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "def agregar_hist_exog_mercado(df_panel, include_vix=True, rv_windows=(5,10,21)):\n",
    "    \"\"\"\n",
    "    hist_exog:\n",
    "      - vix (mismo para todas las series por fecha)\n",
    "      - rv_*: volatilidad realizada por ticker (rolling std de logret_raw)\n",
    "    \"\"\"\n",
    "    out = df_panel.copy().sort_values([\"unique_id\",\"ds\"])\n",
    "\n",
    "    # VIX\n",
    "    if include_vix:\n",
    "        vix = pdr.DataReader(\"VIXCLS\", \"fred\", START, END)\n",
    "        print(vix)\n",
    "        vix = vix[[\"VIXCLS\"]].reset_index().rename(columns={\"DATE\":\"ds\",\"VIXCLS\":\"vix\"})\n",
    "        vix[\"ds\"] = pd.to_datetime(vix[\"ds\"])\n",
    "        out = out.merge(vix, on=\"ds\", how=\"left\")\n",
    "\n",
    "    else:\n",
    "        out[\"vix\"] = np.nan\n",
    "\n",
    "    # Realized volatility por unique_id\n",
    "    for w in rv_windows:\n",
    "        col = f\"rv_{w}\"\n",
    "        # rolling std sobre logret_raw por serie\n",
    "        out[col] = (\n",
    "            out.groupby(\"unique_id\", sort=False)[\"logret_raw\"]\n",
    "               .transform(lambda s: s.rolling(window=w, min_periods=max(2, w//2)).std())\n",
    "               .astype(float)\n",
    "        )\n",
    "    return out\n",
    "\n",
    "\n",
    "def agregar_stat_exog(df_panel, sector_map):\n",
    "    \"\"\"stat_exog: sector constante por unique_id.\"\"\"\n",
    "    out = df_panel.copy()\n",
    "    out[\"sector\"] = out[\"unique_id\"].map(sector_map).fillna(\"Unknown\")\n",
    "    return out\n",
    "\n",
    "\n",
    "def finalizar_panel(df):\n",
    "    \"\"\"\n",
    "    Ordena y selecciona columnas finales:\n",
    "      - Core: unique_id, ds, y\n",
    "      - futr_exog: calendario\n",
    "      - hist_exog: vix, rv_*\n",
    "      - stat_exog: sector\n",
    "      - Auditoría: Close, Volume, logret_raw\n",
    "    \"\"\"\n",
    "    df = df.rename(columns={\"logret_scaled\": \"y\"})\n",
    "    core = [\"unique_id\",\"ds\",\"y\"]\n",
    "\n",
    "    futr = [\"dow\",\"dom\",\"woy\",\"month\",\"qtr\",\"eom\",\"eoq\",\"eoy\"]\n",
    "    futr = [c if c in df.columns else None for c in futr]\n",
    "    futr = [c for c in futr if c is not None]\n",
    "\n",
    "    hist = [\"vix\"] + [c for c in df.columns if c.startswith(\"rv_\")]\n",
    "    # stat = [\"sector\"]\n",
    "    audit = [c for c in [\"Close\",\"Volume\",\"logret_raw\"] if c in df.columns]\n",
    "\n",
    "    ordered = core + futr + hist + audit\n",
    "    ordered = [c for c in ordered if c in df.columns]\n",
    "\n",
    "    out = df[ordered].copy()\n",
    "    out = out.sort_values([\"unique_id\",\"ds\"]).reset_index(drop=True)\n",
    "\n",
    "    # Tipos\n",
    "    out[\"unique_id\"] = out[\"unique_id\"].astype(str)\n",
    "    out[\"ds\"] = pd.to_datetime(out[\"ds\"])\n",
    "    out[\"y\"] = out[\"y\"].astype(float)\n",
    "    return out\n",
    "\n",
    "\n",
    "def construir_panel():\n",
    "    # 1) Descarga wide (1 col por ticker)\n",
    "    closes_wide = download_adjusted_closes(\n",
    "        TICKERS,\n",
    "        START,\n",
    "        END,\n",
    "        auto_adjust=AUTO_ADJUST,  # correcto\n",
    "        threads=True              # NO pasar BACK_ADJUST aquí\n",
    "    )\n",
    "\n",
    "    # 2) wide -> long (unique_id, ds, Close)\n",
    "    precios = closes_wide_to_long_panel(closes_wide)\n",
    "\n",
    "    # 3) Log-returns + escalado robusto\n",
    "    ret = calcular_logret(precios, price_col=\"Close\")\n",
    "\n",
    "    # 4) Exógenas de calendario (futr_exog)\n",
    "    ret = agregar_calendario(ret)\n",
    "\n",
    "    # 5) Exógenas históricas (VIX + realized vol)\n",
    "    ret = agregar_hist_exog_mercado(ret, include_vix=INCLUDE_VIX, rv_windows=RV_WINDOWS)\n",
    "\n",
    "    # 6) Exógenas estáticas por serie (sector)\n",
    "    ret = agregar_stat_exog(ret, SECTOR_MAP)\n",
    "\n",
    "    # 7) Finalizar columnas y limpieza básica\n",
    "    panel = finalizar_panel(ret)\n",
    "    panel = panel.dropna(subset=[\"y\"]).reset_index(drop=True)\n",
    "    return panel\n",
    "\n",
    "\n",
    "def guardar_panel(df):\n",
    "    asegurar_directorio_salida(OUT_DIR)\n",
    "    ruta = os.path.join(OUT_DIR, OUT_PARQUET)\n",
    "    df.to_parquet(ruta, index=False)\n",
    "    return ruta\n",
    "\n",
    "\n",
    "def imprimir_esquema_y_listas(df):\n",
    "    # Esquema\n",
    "    esquema = pd.DataFrame({\"column\": df.columns, \"dtype\": [str(df[c].dtype) for c in df.columns]})\n",
    "    print(\"Schema (columna, dtype):\")\n",
    "    print(esquema.to_string(index=False))\n",
    "\n",
    "    # Listas sugeridas para NBEATSx\n",
    "    futr_exog = [c for c in [\"dow\",\"dom\",\"woy\",\"month\",\"qtr\",\"eom\",\"eoq\",\"eoy\",\"is_month_start\",\"is_quarter_start\",\"is_year_start\"] if c in df.columns]\n",
    "    hist_exog = [\"vix\"] + [c for c in df.columns if c.startswith(\"rv_\")]\n",
    "    stat_exog = [\"sector\"]\n",
    "\n",
    "    # print(\"\\nSugerencia de exógenas para NeuralForecast.NBEATSx:\")\n",
    "    # print(\"  futr_exog =\", futr_exog)\n",
    "    # print(\"  hist_exog =\", hist_exog)\n",
    "    # print(\"  stat_exog =\", stat_exog)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "753c6a82",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m panel = \u001b[43mconstruir_panel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m ruta = guardar_panel(panel)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPanel estilo Nixtla guardado en:\u001b[39m\u001b[33m\"\u001b[39m, ruta)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 204\u001b[39m, in \u001b[36mconstruir_panel\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconstruir_panel\u001b[39m():\n\u001b[32m    203\u001b[39m     \u001b[38;5;66;03m# 1) Descarga wide (1 col por ticker)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m204\u001b[39m     closes_wide = \u001b[43mdownload_adjusted_closes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m        \u001b[49m\u001b[43mTICKERS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m        \u001b[49m\u001b[43mSTART\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m        \u001b[49m\u001b[43mEND\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m        \u001b[49m\u001b[43mauto_adjust\u001b[49m\u001b[43m=\u001b[49m\u001b[43mAUTO_ADJUST\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# correcto\u001b[39;49;00m\n\u001b[32m    209\u001b[39m \u001b[43m        \u001b[49m\u001b[43mthreads\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m              \u001b[49m\u001b[38;5;66;43;03m# NO pasar BACK_ADJUST aquí\u001b[39;49;00m\n\u001b[32m    210\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    212\u001b[39m     \u001b[38;5;66;03m# 2) wide -> long (unique_id, ds, Close)\u001b[39;00m\n\u001b[32m    213\u001b[39m     precios = closes_wide_to_long_panel(closes_wide)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mdownload_adjusted_closes\u001b[39m\u001b[34m(tickers, start, end, auto_adjust, threads)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tickers:\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mLa lista de tickers no puede estar vacía.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m data = \u001b[43myf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtickers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauto_adjust\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauto_adjust\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroup_by\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mticker\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mthreads\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthreads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tickers, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tickers) == \u001b[32m1\u001b[39m:\n\u001b[32m     34\u001b[39m     t = tickers[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tickers, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m tickers\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\yfinance\\utils.py:92\u001b[39m, in \u001b[36mlog_indent_decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     89\u001b[39m logger.debug(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mEntering \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m()\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m IndentationContext():\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m logger.debug(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mExiting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m()\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\yfinance\\multi.py:167\u001b[39m, in \u001b[36mdownload\u001b[39m\u001b[34m(tickers, start, end, actions, threads, ignore_tz, group_by, auto_adjust, back_adjust, repair, keepna, progress, period, interval, prepost, proxy, rounding, timeout, session, multi_level_index)\u001b[39m\n\u001b[32m    160\u001b[39m         _download_one_threaded(ticker, period=period, interval=interval,\n\u001b[32m    161\u001b[39m                                start=start, end=end, prepost=prepost,\n\u001b[32m    162\u001b[39m                                actions=actions, auto_adjust=auto_adjust,\n\u001b[32m    163\u001b[39m                                back_adjust=back_adjust, repair=repair, keepna=keepna,\n\u001b[32m    164\u001b[39m                                progress=(progress \u001b[38;5;129;01mand\u001b[39;00m i > \u001b[32m0\u001b[39m),\n\u001b[32m    165\u001b[39m                                rounding=rounding, timeout=timeout)\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(shared._DFS) < \u001b[38;5;28mlen\u001b[39m(tickers):\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m         \u001b[43m_time\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[38;5;66;03m# download synchronously\u001b[39;00m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    170\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, ticker \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tickers):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "panel = construir_panel()\n",
    "ruta = guardar_panel(panel)\n",
    "print(\"Panel estilo Nixtla guardado en:\", ruta)\n",
    "# imprimir_esquema_y_listas(panel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f02fc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['unique_id', 'ds', 'y', 'dow', 'dom', 'woy', 'month', 'qtr', 'eom',\n",
       "       'eoq', 'eoy', 'vix', 'rv_5', 'rv_10', 'rv_21', 'Close', 'logret_raw'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prueba = pd.read_parquet(r\"C:\\Users\\HP\\Downloads\\Tareas y actividades Tec\\Experimento_2_NIXTLA\\data\\market_panel.parquet\")\n",
    "prueba.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689a29a5",
   "metadata": {},
   "source": [
    "# Agregar TDA\n",
    "\n",
    "En esta sección buscaremos hacer las pruebas de la "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9939a672",
   "metadata": {},
   "outputs": [],
   "source": [
    "def takens_embedding_1d(series, m, tau):\n",
    "    \"\"\"\n",
    "    Construye la incrustación de Takens para una serie 1D.\n",
    "\n",
    "    Parámetros\n",
    "    ----------\n",
    "    series : array-like (1D)\n",
    "        Serie temporal (ej. logret_scaled) como np.ndarray.\n",
    "    m : int\n",
    "        Dimensión de incrustación.\n",
    "    tau : int\n",
    "        Retardo (lag) entre coordenadas.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    emb : np.ndarray of shape (n_points, m)\n",
    "        Nube de puntos embebida. Si la serie es demasiado corta,\n",
    "        devuelve un array vacío de shape (0, m).\n",
    "    \"\"\"\n",
    "    series = np.asarray(series, dtype=float)\n",
    "    n = series.shape[0]\n",
    "    delay_span = (m - 1) * tau\n",
    "\n",
    "    if n <= delay_span:\n",
    "        return np.empty((0, m))\n",
    "\n",
    "    n_points = n - delay_span\n",
    "    emb = np.empty((n_points, m), dtype=float)\n",
    "\n",
    "    for i in range(n_points):\n",
    "        for j in range(m):\n",
    "            emb[i, j] = series[i + j * tau]\n",
    "\n",
    "    return emb\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 2. Diagrama de persistencia H1 con Gudhi\n",
    "# =========================\n",
    "\n",
    "# def compute_h1_diagram_rips(points, maxdim=1):\n",
    "#     \"\"\"\n",
    "#     Calcula el diagrama de persistencia en homología 1 (H1) usando\n",
    "#     un complejo de Rips de Gudhi.\n",
    "\n",
    "#     Parámetros\n",
    "#     ----------\n",
    "#     points : np.ndarray, shape (n_points, n_dims)\n",
    "#         Nube de puntos en R^d.\n",
    "#     maxdim : int\n",
    "#         Dimensión máxima del complejo de Rips (>= 1).\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     diag_h1 : np.ndarray, shape (n_features, 2)\n",
    "#         Arreglo con [birth, death] de cada clase en H1.\n",
    "#         Si no hay puntos o no hay ciclos, devuelve shape (0, 2).\n",
    "#     \"\"\"\n",
    "#     points = np.asarray(points, dtype=float)\n",
    "\n",
    "#     if points.ndim != 2 or points.shape[0] < 2:\n",
    "#         # Con menos de 2 puntos no hay nada interesante que calcular\n",
    "#         return np.empty((0, 2))\n",
    "\n",
    "#     rips_complex = gd.RipsComplex(points=points)\n",
    "#     simplex_tree = rips_complex.create_simplex_tree(max_dimension=maxdim)\n",
    "\n",
    "#     # homology_coeff_field=2 es el default; min_persistence se podría tunear\n",
    "#     simplex_tree.compute_persistence()\n",
    "\n",
    "#     diag_h1 = simplex_tree.persistence_intervals_in_dimension(1)\n",
    "\n",
    "#     if len(diag_h1) == 0:\n",
    "#         return np.empty((0, 2))\n",
    "\n",
    "#     diag_h1 = np.asarray(diag_h1, dtype=float)\n",
    "\n",
    "#     # Filtrar muertes infinitas, si las hubiera\n",
    "#     finite_mask = np.isfinite(diag_h1[:, 1])\n",
    "#     diag_h1 = diag_h1[finite_mask]\n",
    "\n",
    "#     if diag_h1.shape[0] == 0:\n",
    "#         return np.empty((0, 2))\n",
    "\n",
    "#     return diag_h1\n",
    "\n",
    "def compute_h1_diagram_rips(points, maxdim=2):\n",
    "    \"\"\"\n",
    "    Calcula el diagrama de persistencia en homología 1 (H1) usando\n",
    "    un complejo de Rips de Gudhi.\n",
    "    \"\"\"\n",
    "    points = np.asarray(points, dtype=float)\n",
    "\n",
    "    if points.ndim != 2 or points.shape[0] < 2:\n",
    "        return np.empty((0, 2))\n",
    "\n",
    "    # AHORA max_dimension=2 para que se construyan triángulos\n",
    "    rips_complex = gd.RipsComplex(points=points)\n",
    "    simplex_tree = rips_complex.create_simplex_tree(max_dimension=maxdim)\n",
    "\n",
    "    simplex_tree.compute_persistence()\n",
    "    diag_h1 = simplex_tree.persistence_intervals_in_dimension(1)\n",
    "\n",
    "    if len(diag_h1) == 0:\n",
    "        return np.empty((0, 2))\n",
    "\n",
    "    diag_h1 = np.asarray(diag_h1, dtype=float)\n",
    "\n",
    "    # El resto lo dejamos igual por esta prueba\n",
    "    finite_mask = np.isfinite(diag_h1[:, 1])\n",
    "    diag_h1 = diag_h1[finite_mask]\n",
    "\n",
    "    if diag_h1.shape[0] == 0:\n",
    "        return np.empty((0, 2))\n",
    "\n",
    "    return diag_h1\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 3. Estadísticos TDA sobre un diagrama\n",
    "# =========================\n",
    "\n",
    "def persistent_entropy(diagram, eps=1e-12):\n",
    "    \"\"\"\n",
    "    Entropía persistente de un diagrama de persistencia.\n",
    "\n",
    "    Definición:\n",
    "    - Sea d_i = death_i - birth_i > 0\n",
    "    - L = sum_i d_i\n",
    "    - p_i = d_i / L\n",
    "    - H = - sum_i p_i * log(p_i)\n",
    "\n",
    "    Si el diagrama está vacío o L <= 0, devuelve 0.0.\n",
    "\n",
    "    Parámetros\n",
    "    ----------\n",
    "    diagram : np.ndarray, shape (n_features, 2)\n",
    "        Diagrama de persistencia [birth, death].\n",
    "    eps : float\n",
    "        Pequeño valor para evitar log(0).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    h : float\n",
    "        Entropía persistente (log natural).\n",
    "    \"\"\"\n",
    "    if diagram.size == 0:\n",
    "        return 0.0\n",
    "\n",
    "    lifetimes = diagram[:, 1] - diagram[:, 0]\n",
    "    lifetimes = lifetimes[lifetimes > 0.0]\n",
    "\n",
    "    if lifetimes.size == 0:\n",
    "        return 0.0\n",
    "\n",
    "    L = np.sum(lifetimes)\n",
    "    if L <= 0.0:\n",
    "        return 0.0\n",
    "\n",
    "    p = lifetimes / L\n",
    "    p = p[p > 0.0]\n",
    "\n",
    "    if p.size == 0:\n",
    "        return 0.0\n",
    "\n",
    "    h = -np.sum(p * np.log(p + eps))\n",
    "    return float(h)\n",
    "\n",
    "\n",
    "def diagram_amplitude(diagram, p=2.0):\n",
    "    \"\"\"\n",
    "    Amplitud de un diagrama de persistencia.\n",
    "\n",
    "    Implementación estándar:\n",
    "    - Distancia de cada punto a la diagonal en norma L∞ es d_i / 2,\n",
    "      donde d_i = death_i - birth_i.\n",
    "    - Definimos la amplitud como:\n",
    "          A_p = ( sum_i (d_i / 2)^p )^(1/p)\n",
    "\n",
    "    Si el diagrama está vacío, devuelve 0.0.\n",
    "\n",
    "    Parámetros\n",
    "    ----------\n",
    "    diagram : np.ndarray, shape (n_features, 2)\n",
    "        Diagrama de persistencia [birth, death].\n",
    "    p : float\n",
    "        Parámetro de la norma (habitualmente p=2).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A_p : float\n",
    "        Amplitud del diagrama.\n",
    "    \"\"\"\n",
    "    if diagram.size == 0:\n",
    "        return 0.0\n",
    "\n",
    "    lifetimes = diagram[:, 1] - diagram[:, 0]\n",
    "    lifetimes = lifetimes[lifetimes > 0.0]\n",
    "\n",
    "    if lifetimes.size == 0:\n",
    "        return 0.0\n",
    "\n",
    "    distances = lifetimes / 2.0  # distancia a la diagonal\n",
    "    if p <= 0:\n",
    "        # Por seguridad, tratamos p<=0 como caso límite: usar máximo\n",
    "        return float(np.max(distances))\n",
    "\n",
    "    A_p = np.sum(distances ** p) ** (1.0 / p)\n",
    "    return float(A_p)\n",
    "\n",
    "\n",
    "def diagram_num_points(diagram):\n",
    "    \"\"\"\n",
    "    Número de puntos en el diagrama (cardinalidad).\n",
    "\n",
    "    Parámetros\n",
    "    ----------\n",
    "    diagram : np.ndarray, shape (n_features, 2)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    n_points : int\n",
    "    \"\"\"\n",
    "    return int(diagram.shape[0])\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 4. Features TDA para UNA ventana\n",
    "# =========================\n",
    "\n",
    "def tda_features_for_window(window_values, m=4, tau=4, maxdim=1):\n",
    "    \"\"\"\n",
    "    Calcula entropía persistente, amplitud y número de puntos\n",
    "    sobre H1 de una ventana temporal de la serie (logret_scaled).\n",
    "\n",
    "    Parámetros\n",
    "    ----------\n",
    "    window_values : array-like (1D)\n",
    "        Segmento de la serie temporal.\n",
    "    m : int\n",
    "        Dimensión de Takens.\n",
    "    tau : int\n",
    "        Retardo de Takens.\n",
    "    maxdim : int\n",
    "        Dimensión máxima del complejo de Rips (>=1).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    entropy : float\n",
    "    amplitude : float\n",
    "    n_points : int\n",
    "        Si no se pueden calcular (ej. ventana muy corta o NaNs),\n",
    "        devuelve (np.nan, np.nan, np.nan).\n",
    "    \"\"\"\n",
    "    window_values = np.asarray(window_values, dtype=float)\n",
    "\n",
    "    # Si hay NaNs, preferimos no calcular nada\n",
    "    if np.isnan(window_values).any():\n",
    "        return np.nan, np.nan, np.nan\n",
    "\n",
    "    emb = takens_embedding_1d(window_values, m=m, tau=tau)\n",
    "\n",
    "    if emb.shape[0] == 0:\n",
    "        return np.nan, np.nan, np.nan\n",
    "\n",
    "    diag_h1 = compute_h1_diagram_rips(emb, maxdim=maxdim)\n",
    "\n",
    "    # Si el diagrama es vacío, devolvemos 0.0 para las métricas (sin NaNs)\n",
    "    if diag_h1.shape[0] == 0:\n",
    "        return 0.0, 0.0, 0.0\n",
    "\n",
    "    entropy = persistent_entropy(diag_h1)\n",
    "    amplitude = diagram_amplitude(diag_h1, p=2.0)\n",
    "    n_points = diagram_num_points(diag_h1)\n",
    "\n",
    "    return entropy, amplitude, float(n_points)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 5. Aplicar TDA al panel Nixtla (ventana 21 días, H1)\n",
    "# =========================\n",
    "\n",
    "def add_tda_helpers_to_panel(\n",
    "    df,\n",
    "    value_col=\"logret_scaled\",\n",
    "    id_col=\"unique_id\",\n",
    "    time_col=\"ds\",\n",
    "    window_size=21,\n",
    "    m=4,\n",
    "    tau=4\n",
    "):\n",
    "    \"\"\"\n",
    "    Agrega 3 columnas TDA al panel Nixtla, calculadas a partir de\n",
    "    logret_scaled usando ventana deslizante de 21 días y homología H1.\n",
    "\n",
    "    Nuevas columnas:\n",
    "    - 'tda_entropy_h1_w21'\n",
    "    - 'tda_amplitude_h1_w21'\n",
    "    - 'tda_n_points_h1_w21'\n",
    "\n",
    "    Parámetros\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame con columnas tipo Nixtla [unique_id, ds, y, ..., logret_scaled].\n",
    "    value_col : str\n",
    "        Nombre de la columna sobre la que se calcula Takens (ej. 'logret_scaled').\n",
    "    id_col : str\n",
    "        Columna de identificador de serie (ej. 'unique_id').\n",
    "    time_col : str\n",
    "        Columna temporal (ej. 'ds').\n",
    "    window_size : int\n",
    "        Tamaño de la ventana deslizante (21).\n",
    "    m : int\n",
    "        Dimensión de Takens.\n",
    "    tau : int\n",
    "        Retardo de Takens.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_out : pd.DataFrame\n",
    "        Nuevo DataFrame con las columnas TDA añadidas.\n",
    "    \"\"\"\n",
    "    # Copia para no modificar el original in-place\n",
    "    df = df.copy()\n",
    "\n",
    "    # Ordenamos por panel y tiempo para asegurar causalidad\n",
    "    df = df.sort_values([id_col, time_col]).reset_index(drop=True)\n",
    "\n",
    "    # Inicializamos columnas de salida\n",
    "    df[\"tda_entropy_h1_w21\"] = np.nan\n",
    "    df[\"tda_amplitude_h1_w21\"] = np.nan\n",
    "    df[\"tda_n_points_h1_w21\"] = np.nan\n",
    "\n",
    "    # Procesamos serie por serie (por ticker)\n",
    "    for uid, group_idx in df.groupby(id_col).groups.items():\n",
    "        idx_array = np.asarray(list(group_idx))\n",
    "        series_values = df.loc[idx_array, value_col].to_numpy(dtype=float)\n",
    "\n",
    "        n = series_values.shape[0]\n",
    "\n",
    "        ent_values = np.full(n, np.nan, dtype=float)\n",
    "        amp_values = np.full(n, np.nan, dtype=float)\n",
    "        npts_values = np.full(n, np.nan, dtype=float)\n",
    "\n",
    "        for i in range(window_size - 1, n):\n",
    "            window = series_values[i - window_size + 1 : i + 1]\n",
    "\n",
    "            e, a, npts = tda_features_for_window(\n",
    "                window,\n",
    "                m=m,\n",
    "                tau=tau,\n",
    "                maxdim=2\n",
    "            )\n",
    "\n",
    "            ent_values[i] = e\n",
    "            amp_values[i] = a\n",
    "            npts_values[i] = npts\n",
    "\n",
    "        # Asignamos de vuelta las columnas a las filas correspondientes\n",
    "        df.loc[idx_array, \"tda_entropy_h1_w21\"] = ent_values\n",
    "        df.loc[idx_array, \"tda_amplitude_h1_w21\"] = amp_values\n",
    "        df.loc[idx_array, \"tda_n_points_h1_w21\"] = npts_values\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474b381f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df = \u001b[43mpd\u001b[49m.read_parquet(\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mC:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mUsers\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mHP\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mDownloads\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mTareas y actividades Tec\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mExperimento_2_NIXTLA\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mmarket_panel.parquet\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# (Si logret_scaled ya existe, no necesitas esto; es solo un ejemplo)\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# df[\"logret_scaled\"] = ...\u001b[39;00m\n\u001b[32m      5\u001b[39m \n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Agregar helpers TDA\u001b[39;00m\n\u001b[32m      7\u001b[39m df_tda = add_tda_helpers_to_panel(\n\u001b[32m      8\u001b[39m     df,\n\u001b[32m      9\u001b[39m     value_col=\u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m     tau=\u001b[32m4\u001b[39m\n\u001b[32m     15\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet(r\"C:\\Users\\HP\\Downloads\\Tareas y actividades Tec\\Experimento_2_NIXTLA\\data\\market_panel.parquet\")\n",
    "\n",
    "# (Si logret_scaled ya existe, no necesitas esto; es solo un ejemplo)\n",
    "# df[\"logret_scaled\"] = ...\n",
    "\n",
    "# Agregar helpers TDA\n",
    "df_tda = add_tda_helpers_to_panel(\n",
    "    df,\n",
    "    value_col=\"y\",\n",
    "    id_col=\"unique_id\",\n",
    "    time_col=\"ds\",\n",
    "    window_size=21,\n",
    "    m=4,\n",
    "    tau=4\n",
    ")\n",
    "\n",
    "# Luego puedes volver a guardar:\n",
    "df_tda.to_parquet(r\"data\\panel_tda.parquet\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
